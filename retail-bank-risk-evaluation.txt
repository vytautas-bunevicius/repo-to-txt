=== setup.py ===
import pathlib
from setuptools import setup, find_packages

HERE = pathlib.Path(__file__).parent
README = (HERE / "README.md").read_text()
REQUIREMENTS = (HERE / "requirements.txt").read_text().splitlines()

setup(
    name="retail_bank_risk",
    version="0.1",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    install_requires=REQUIREMENTS,
    description="Retail Bank Risk Evaluation",
    long_description=README,
    long_description_content_type="text/markdown",
)


=== src/retail_bank_risk/__init__.py ===


=== src/retail_bank_risk/advanced_visualizations_utils.py ===
# cspell:disable
# pylint: disable=line-too-long

"""
This module contains a collection of functions for visualizing and analyzing machine learning model performance,
including SHAP values, model comparison metrics, and confusion matrices.

The visualizations are created using Plotly and are designed to be informative, aesthetically pleasing,
and easily interpretable. The module also supports saving these visualizations as images.

Key functionalities include:
- `shap_summary_plot`: Creates a bar plot to display SHAP feature importance.
- `shap_force_plot`: Generates a waterfall plot to visualize individual SHAP values and their impact on predictions.
- `plot_model_performance`: Plots and compares performance metrics across different models using a grouped bar chart.
- `plot_combined_confusion_matrices`: Creates confusion matrices for multiple models, visualizing their classification performance.

Constants defined:
- `BACKGROUND_COLOR`: The background color used for all plots.
- `PRIMARY_COLORS`: A palette of primary colors used for the main elements of the visualizations.
- `PLOT_COLORS`: A subset of colors used specifically for plotting.
- `SECONDARY_COLORS`: A palette of secondary colors for additional elements.
- `ALL_COLORS`: A combination of primary and secondary colors for use across different plots.

This module is intended for use in data analysis workflows where model interpretability and performance
visualization are essential. The plots generated can be displayed interactively or saved for reporting purposes.
"""

from typing import List, Optional, Dict, Any

import numpy as np
import pandas as pd

from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve
from sklearn.model_selection import learning_curve

import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# Constants
BACKGROUND_COLOR = "#EEECE2"
PRIMARY_COLORS = ["#CC7B5C", "#D4A27F", "#EBDBBC", "#9C8AA5"]
PLOT_COLORS = ["#91A694", "#9C8AA5", "#CC7B5C"]
SECONDARY_COLORS = [
    "#91A694",
    "#8B9BAE",
    "#666663",
    "#BFBFBA",
    "#E5E4DF",
    "#F0F0EB",
    "#FAFAF7",
]
ALL_COLORS = PRIMARY_COLORS + SECONDARY_COLORS


def shap_summary_plot(
    shap_values: np.ndarray,
    feature_names: List[str],
    save_path: Optional[str] = None,
) -> None:
    """Creates a bar plot of SHAP feature importance.

    Args:
        shap_values (np.ndarray): The SHAP values for each feature.
        feature_names (List[str]): The names of the features.
        save_path (Optional[str]): The file path to save the plot image. Defaults to None.

    Returns:
        None: The function displays the plot and optionally saves it.
    """
    shap_mean = np.abs(shap_values).mean(axis=0)
    feature_importance = pd.DataFrame(
        {"feature": feature_names, "importance": shap_mean}
    )
    feature_importance = feature_importance.sort_values(
        "importance", ascending=True
    )

    fig = px.bar(
        feature_importance,
        x="importance",
        y="feature",
        orientation="h",
        title="SHAP Feature Importance",
        labels={"importance": "mean(|SHAP value|)", "feature": "Feature"},
        color="importance",
        color_continuous_scale=PRIMARY_COLORS,
    )

    fig.update_layout(
        height=1200,
        width=1200,
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        font=dict(family="Styrene A", size=12, color="#191919"),
        title=dict(
            text="SHAP Feature Importance",
            x=0.5,
            xanchor="center",
            font=dict(family="Styrene B", size=20, color="#191919"),
        ),
    )

    if save_path:
        fig.write_image(save_path)


def shap_force_plot(
    shap_data: Dict[str, Any],
    explainer: Any,
    idx: int = 0,
    save_path: Optional[str] = None,
) -> None:
    """
    Generates a waterfall plot to visualize individual SHAP values and their impact on predictions.

    Args:
        shap_data (Dict[str, Any]): A dictionary containing 'shap_values', 'x_data', and 'feature_names'.
        explainer (Any): The SHAP explainer object.
        idx (int, optional): The index of the instance to plot. Defaults to 0.
        save_path (Optional[str]): The file path to save the plot as an image. Defaults to None.

    Returns:
        None: This function does not return anything. It plots the SHAP values using Plotly.
    """
    shap_values = shap_data["shap_values"]
    x_data = shap_data["x_data"]
    feature_names = shap_data["feature_names"]

    features = pd.DataFrame(
        {
            "feature": feature_names,
            "value": x_data[idx],
            "shap": shap_values[idx],
        }
    )
    features = features.sort_values("shap", key=abs, ascending=False)

    base_value = getattr(explainer, "expected_value", None)
    if base_value is None:
        raise AttributeError(
            "The explainer object does not have an 'expected_value' attribute."
        )

    fig = go.Figure(
        go.Waterfall(
            name="20",
            orientation="h",
            measure=["relative"] * len(features) + ["total"],
            x=list(features.loc[:, "shap"]) + [base_value],
            textposition="outside",
            text=[
                f"{feat}: {val:.2f}"
                for _, row in features.iterrows()
                for feat, val in zip(row["feature"], row["value"])
            ]
            + ["Base value"],
            y=list(features.loc[:, "feature"]) + ["Base value"],
            connector={"line": {"color": "#666663"}},
            decreasing={"marker": {"color": PRIMARY_COLORS[0]}},
            increasing={"marker": {"color": PRIMARY_COLORS[1]}},
        )
    )

    fig.update_layout(
        height=800,
        width=800,
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        font=dict(family="Styrene A", size=12, color="#191919"),
        title=dict(font=dict(family="Styrene B", size=20, color="#191919")),
    )
    if save_path:
        fig.write_image(save_path)


def plot_model_performance(
    results: Dict[str, Dict[str, float]],
    metrics: List[str],
    save_path: Optional[str] = None,
) -> None:
    """Plots and optionally saves a bar chart of model performance metrics with legend on the right.

    Args:
        results: A dictionary with model names as keys and dicts of performance metrics as values.
        metrics: List of performance metrics to plot (e.g., 'Accuracy', 'Precision').
        save_path: Optional path to save the plot image.

    Returns:
        None. Displays the plot and optionally saves it to a file.
    """
    model_names = list(results.keys())
    data = {
        metric: [results[name][metric] for name in model_names]
        for metric in metrics
    }

    fig = go.Figure()

    for i, metric in enumerate(metrics):
        fig.add_trace(
            go.Bar(
                x=model_names,
                y=data[metric],
                name=metric,
                marker_color=ALL_COLORS[i % len(ALL_COLORS)],
                text=[f"{value:.2f}" for value in data[metric]],
                textposition="auto",
            )
        )

    axis_font = {"family": "Styrene A", "color": "#191919"}

    fig.update_layout(
        barmode="group",
        title={
            "text": "Comparison of Model Performance Metrics",
            "y": 0.95,
            "x": 0.5,
            "xanchor": "center",
            "yanchor": "top",
            "font": {"family": "Styrene B", "size": 24, "color": "#191919"},
        },
        xaxis_title="Model",
        yaxis_title="Value",
        legend_title="Metrics",
        font={**axis_font, "size": 14},
        height=500,
        width=1200,
        template="plotly_white",
        legend={"yanchor": "top", "y": 1, "xanchor": "left", "x": 1.02},
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
    )

    fig.update_yaxes(
        range=[0, 1], showgrid=True, gridwidth=1, gridcolor="LightGrey"
    )
    fig.update_xaxes(tickangle=-45, tickfont={**axis_font, "size": 12})

    fig.show()

    if save_path:
        fig.write_image(save_path)


def plot_combined_confusion_matrices(
    results: Dict[str, Dict[str, float]],
    y_test: np.ndarray,
    y_pred_dict: Dict[str, np.ndarray],
    labels: Optional[List[str]] = None,
    save_path: Optional[str] = None,
) -> None:
    """Plots combined confusion matrices for multiple models.

    Args:
        results (Dict[str, Dict[str, float]]): A dictionary containing the results of each model.
            The keys are the model names, and the values are dictionaries containing the model's performance metrics.
        y_test (np.ndarray): The true labels of the test data.
        y_pred_dict (Dict[str, np.ndarray]): A dictionary containing the predicted labels for each model.
            The keys are the model names, and the values are the predicted labels.
        labels (Optional[List[str]], optional): A list of labels for the classes. Defaults to None.
        save_path (Optional[str], optional): The file path to save the plot as an image. Defaults to None.

    Returns:
        None: This function does not return anything. It plots the confusion matrices using Plotly.

    Raises:
        None: This function does not raise any exceptions.

    """
    n_models = len(results)

    if n_models <= 2:
        rows, cols = 1, 2
    else:
        rows, cols = 2, 2

    fig = make_subplots(
        rows=rows,
        cols=cols,
        subplot_titles=list(results.keys()) + [""] * (rows * cols - n_models),  # type: ignore
        vertical_spacing=0.2,
        horizontal_spacing=0.1,
    )

    axis_font = {"family": "Styrene A", "color": "#191919"}

    for i, name in enumerate(results.keys()):  # type: ignore
        row = i // cols + 1
        col = i % cols + 1

        cm = confusion_matrix(y_test, y_pred_dict[name])  # type: ignore
        cm_percent = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] * 100

        text = [
            [
                f"TN: {cm[0][0]}<br>({cm_percent[0][0]:.1f}%)",
                f"FP: {cm[0][1]}<br>({cm_percent[0][1]:.1f}%)",
            ],
            [
                f"FN: {cm[1][0]}<br>({cm_percent[1][0]:.1f}%)",
                f"TP: {cm[1][1]}<br>({cm_percent[1][1]:.1f}%)",
            ],
        ]

        colorscale = [
            [0, PRIMARY_COLORS[2]],
            [0.33, PRIMARY_COLORS[1]],
            [0.66, PRIMARY_COLORS[1]],
            [1, PRIMARY_COLORS[0]],
        ]

        heatmap = go.Heatmap(
            z=cm,
            x=labels or ["No Stroke", "Stroke"],  # type: ignore
            y=labels or ["No Stroke", "Stroke"],  # type: ignore
            hoverongaps=False,
            text=text,
            texttemplate="%{text}",
            colorscale=colorscale,
            showscale=False,
        )

        fig.add_trace(heatmap, row=row, col=col)

        fig.update_xaxes(
            title_text="Predicted",
            row=row,
            col=col,
            tickfont={**axis_font, "size": 10},
            title_standoff=25,
        )
        fig.update_yaxes(
            title_text="Actual",
            row=row,
            col=col,
            tickfont={**axis_font, "size": 10},
            title_standoff=25,
        )

    # Adjust layout based on number of models
    height = 600 if n_models <= 2 else 1000
    width = 1200

    fig.update_layout(
        title_text="Confusion Matrices for All Models",
        title_x=0.5,
        title_font={"family": "Styrene B", "size": 24, "color": "#191919"},
        height=height,
        width=width,
        showlegend=False,
        font={**axis_font, "size": 12},
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        margin=dict(t=100, b=50, l=50, r=50),
    )

    # Adjust subplot titles
    for i in fig["layout"]["annotations"]:
        i["font"] = dict(size=16, family="Styrene B", color="#191919")
        i["y"] = i["y"] + 0.03

    fig.show()

    if save_path:  # type: ignore
        fig.write_image(save_path)  # type: ignore


def plot_roc_curve(
    y_true: np.ndarray,
    y_pred_proba: np.ndarray,
    save_path: Optional[str] = None,
) -> None:
    """
    Plot and optionally save the Receiver Operating Characteristic (ROC) curve.

    Args:
        y_true: Array of true labels.
        y_pred_proba: Array of predicted probabilities.
        save_path: Optional path to save the plot image.
    """
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    fig = go.Figure()
    fig.add_trace(
        go.Scatter(
            x=fpr,
            y=tpr,
            mode="lines",
            name="ROC curve",
            line=dict(color=PRIMARY_COLORS[0]),
        )
    )
    fig.add_trace(
        go.Scatter(
            x=[0, 1],
            y=[0, 1],
            mode="lines",
            name="Random",
            line=dict(dash="dash", color=PRIMARY_COLORS[1]),
        )
    )
    fig.update_layout(
        title="Receiver Operating Characteristic (ROC) Curve",
        xaxis_title="False Positive Rate",
        yaxis_title="True Positive Rate",
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        width=1200,
        height=500,
    )
    fig.show()
    if save_path:
        fig.write_image(save_path)


def plot_precision_recall_curve(
    y_true: np.ndarray,
    y_pred_proba: np.ndarray,
    save_path: Optional[str] = None,
) -> None:
    """
    Plot and optionally save the Precision-Recall curve.

    Args:
        y_true: Array of true labels.
        y_pred_proba: Array of predicted probabilities.
        save_path: Optional path to save the plot image.
    """
    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
    fig = go.Figure()
    fig.add_trace(
        go.Scatter(
            x=recall,
            y=precision,
            mode="lines",
            name="PR curve",
            line=dict(color=PRIMARY_COLORS[0]),
        )
    )
    fig.update_layout(
        title="Precision-Recall Curve",
        xaxis_title="Recall",
        yaxis_title="Precision",
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        width=1200,
        height=500,
    )
    fig.show()
    if save_path:
        fig.write_image(save_path)


def plot_confusion_matrix(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    labels: Optional[List[str]] = None,
    save_path: Optional[str] = None,
) -> None:
    """Plots and optionally saves the confusion matrix.

    Args:
        y_true (np.ndarray): Array of true labels.
        y_pred (np.ndarray): Array of predicted labels.
        labels (Optional[List[str]], optional): A list of labels for the classes. Defaults to None.
        save_path (Optional[str], optional): The file path to save the plot as an image. Defaults to None.

    Returns:
        None: This function does not return anything. It plots the confusion matrix using Plotly.

    Raises:
        None: This function does not raise any exceptions.
    """
    cm = confusion_matrix(y_true, y_pred)
    cm_percent = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] * 100

    text = [
        [
            f"TN: {cm[0][0]}<br>({cm_percent[0][0]:.1f}%)",
            f"FP: {cm[0][1]}<br>({cm_percent[0][1]:.1f}%)",
        ],
        [
            f"FN: {cm[1][0]}<br>({cm_percent[1][0]:.1f}%)",
            f"TP: {cm[1][1]}<br>({cm_percent[1][1]:.1f}%)",
        ],
    ]

    colorscale = [
        [0, PRIMARY_COLORS[2]],
        [0.33, PRIMARY_COLORS[1]],
        [0.66, PRIMARY_COLORS[1]],
        [1, PRIMARY_COLORS[0]],
    ]

    fig = go.Figure(
        data=go.Heatmap(
            z=cm,
            x=labels or ["Not Transported", "Transported"],
            y=labels or ["Not Transported", "Transported"],
            hoverongaps=False,
            text=text,
            texttemplate="%{text}",
            colorscale=colorscale,
            showscale=False,
        )
    )

    axis_font = {"family": "Styrene A", "color": "#191919"}

    fig.update_layout(
        title_text="Confusion Matrix",
        title_x=0.5,
        title_font={"family": "Styrene B", "size": 24, "color": "#191919"},
        xaxis_title="Predicted",
        yaxis_title="Actual",
        xaxis=dict(tickfont={**axis_font, "size": 12}),
        yaxis=dict(tickfont={**axis_font, "size": 12}),
        height=500,
        width=1200,
        showlegend=False,
        font={**axis_font, "size": 12},
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        margin=dict(t=100, b=50, l=50, r=50),
    )

    fig.show()

    if save_path:
        fig.write_image(save_path)


def plot_learning_curve(
    estimator,
    features: np.ndarray,
    target: np.ndarray,
    cv: int = 5,
    n_jobs: int = -1,
    train_sizes: np.ndarray = np.linspace(0.1, 1.0, 5),
    save_path: Optional[str] = None,
) -> None:
    """
    Plot and optionally save the learning curve for a given estimator.

    Args:
        estimator: The machine learning model to evaluate.
        features: Feature matrix.
        target: Target vector.
        cv: Number of cross-validation folds.
        n_jobs: Number of jobs to run in parallel.
        train_sizes: Array of training set sizes to evaluate.
        save_path: Optional path to save the plot image.
    """
    train_sizes, train_scores, test_scores = learning_curve(
        estimator,
        features,
        target,
        cv=cv,
        n_jobs=n_jobs,
        train_sizes=train_sizes,
    )

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    fig = go.Figure()
    fig.add_trace(
        go.Scatter(
            x=train_sizes,
            y=train_scores_mean,
            mode="lines+markers",
            name="Training score",
            line=dict(color=PRIMARY_COLORS[0]),
            error_y=dict(type="data", array=train_scores_std, visible=True),
        )
    )
    fig.add_trace(
        go.Scatter(
            x=train_sizes,
            y=test_scores_mean,
            mode="lines+markers",
            name="Cross-validation score",
            line=dict(color=PRIMARY_COLORS[1]),
            error_y=dict(type="data", array=test_scores_std, visible=True),
        )
    )
    fig.update_layout(
        title="Learning Curve",
        xaxis_title="Training examples",
        yaxis_title="Score",
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        width=1200,
        height=500,
    )
    fig.show()
    if save_path:
        fig.write_image(save_path)


=== src/retail_bank_risk/basic_visualizations_utils.py ===
# cspell:disable
# pylint:disable=line-too-long

"""
This module provides a set of functions for creating and displaying various types of plots
to visualize data distributions, feature importances, and correlations using Plotly.

Key functionalities include:
- Plotting histograms, bar charts, and boxplots for specified features.
- Creating correlation matrices for numerical features.
- Visualizing feature importances across different models.
- Comparing data distributions before and after handling missing values.
- Displaying categorical feature distributions by target variables.

Constants:
- `BACKGROUND_COLOR`: The background color used in all plots.
- `PRIMARY_COLORS`: A list of primary colors used for the main elements of the plots.
- `PLOT_COLORS`: A subset of colors used for histogram and bar plot elements.
- `SECONDARY_COLORS`: A set of secondary colors used for additional elements in the plots.
- `ALL_COLORS`: A combination of primary and secondary colors for use across different plots.

Functions:
- `plot_combined_histograms`: Plots histograms for multiple features in a single figure.
- `plot_combined_bar_charts`: Plots bar charts for multiple categorical features.
- `plot_combined_boxplots`: Plots boxplots for multiple numerical features.
- `plot_correlation_matrix`: Plots a correlation matrix for the specified numerical features.
- `plot_feature_importances`: Plots feature importances across different models.
- `plot_distribution_comparison`: Compares feature distributions before and after handling missing values.
- `plot_categorical_features_by_target`: Plots the distribution of categorical features grouped by a target variable.
- `plot_numeric_distributions`: Plots numeric distributions of features grouped by a binary target variable.
- `plot_single_bar_chart`: Plots a percentage bar chart for a single categorical feature.

This module is intended for use in data exploration and analysis workflows, helping to
gain insights into data structure, feature importance, and potential relationships between features.
"""

from typing import List, Optional, Dict

import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Constants
BACKGROUND_COLOR = "#EEECE2"
PRIMARY_COLORS = ["#CC7B5C", "#D4A27F", "#EBDBBC", "#9C8AA5"]
PLOT_COLORS = ["#91A694", "#9C8AA5", "#CC7B5C"]
SECONDARY_COLORS = [
    "#91A694",
    "#8B9BAE",
    "#666663",
    "#BFBFBA",
    "#E5E4DF",
    "#F0F0EB",
    "#FAFAF7",
]
ALL_COLORS = PRIMARY_COLORS + SECONDARY_COLORS


def plot_combined_histograms(
    df: pd.DataFrame,
    features: List[str],
    nbins: int = 40,
    save_path: Optional[str] = None,
) -> None:
    """Plots combined histograms for specified features in the DataFrame.

    Args:
        df: DataFrame containing the features to plot.
        features: List of feature names to plot histograms for.
        nbins: Number of bins for each histogram. Defaults to 40.
        save_path: Optional path to save the plot image.

    Returns:
        None. Displays the plot and optionally saves it to a file.
    """
    title = f"Distribution of {', '.join(features)}"
    rows, cols = 1, len(features)

    fig = make_subplots(rows=rows, cols=cols, horizontal_spacing=0.1)

    axis_font = {"family": "Styrene A", "color": "#191919"}

    for i, feature in enumerate(features):
        fig.add_trace(
            go.Histogram(
                x=df[feature],
                nbinsx=nbins,
                name=feature,
                marker={
                    "color": PRIMARY_COLORS[i % len(PRIMARY_COLORS)],
                    "line": {"color": "#000000", "width": 1},
                },
            ),
            row=1,
            col=i + 1,
        )

        fig.update_xaxes(
            title_text=feature,
            row=1,
            col=i + 1,
            title_standoff=25,
            title_font={**axis_font, "size": 14},
            tickfont={**axis_font, "size": 12},
        )
        fig.update_yaxes(
            title_text="Count",
            row=1,
            col=i + 1,
            title_font={**axis_font, "size": 14},
            tickfont={**axis_font, "size": 12},
        )

    fig.update_layout(
        title_text=title,
        title_x=0.5,
        title_font={"family": "Styrene B", "size": 20, "color": "#191919"},
        showlegend=False,
        template="plotly_white",
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        height=500,
        width=400 * len(features),
        margin={"l": 50, "r": 50, "t": 80, "b": 80},
        font={**axis_font, "size": 12},
    )

    fig.show()

    if save_path:
        fig.write_image(save_path)


def plot_combined_bar_charts(
    df: pd.DataFrame,
    features: List[str],
    save_path: Optional[str] = None,
) -> None:
    """Plots combined bar charts for specified categorical features in the DataFrame.

    Args:
        df: DataFrame containing the features to plot.
        features: List of categorical feature names to plot bar charts for.
        save_path: Optional path to save the plot image.

    Returns:
        None. Displays the plot and optionally saves it to a file.
    """
    title = f"Distribution of {', '.join(features)}"
    rows, cols = 1, len(features)

    fig = make_subplots(rows=rows, cols=cols, horizontal_spacing=0.1)

    axis_font = {"family": "Styrene A", "color": "#191919"}

    for i, feature in enumerate(features):
        value_counts = df[feature].value_counts().reset_index()
        value_counts.columns = [feature, "count"]

        fig.add_trace(
            go.Bar(
                x=value_counts[feature],
                y=value_counts["count"],
                name=feature,
                marker={
                    "color": PRIMARY_COLORS[i % len(PRIMARY_COLORS)],
                    "line": {"color": "#000000", "width": 1},
                },
            ),
            row=1,
            col=i + 1,
        )

        fig.update_xaxes(
            title_text=feature,
            row=1,
            col=i + 1,
            title_standoff=25,
            title_font={**axis_font, "size": 14},
            tickfont={**axis_font, "size": 12},
            showticklabels=True,
        )

        fig.update_yaxes(
            title_text="Count",
            row=1,
            col=i + 1,
            title_font={**axis_font, "size": 14},
            tickfont={**axis_font, "size": 12},
        )

    fig.update_layout(
        title_text=title,
        title_x=0.5,
        title_font={"family": "Styrene B", "size": 20, "color": "#191919"},
        showlegend=False,
        template="plotly_white",
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        height=500,
        width=400 * len(features),
        margin={"l": 50, "r": 50, "t": 80, "b": 150},
        font={**axis_font, "size": 12},
    )

    fig.show()

    if save_path:
        fig.write_image(save_path)


def plot_combined_boxplots(
    df: pd.DataFrame, features: List[str], save_path: Optional[str] = None
) -> None:
    """Plots combined boxplots for specified numerical features in the DataFrame.

    Args:
        df: DataFrame containing the features to plot.
        features: List of numerical feature names to plot boxplots for.
        save_path: Optional path to save the plot image.

    Returns:
        None. Displays the plot and optionally saves it to a file.
    """
    title = f"Boxplots of {', '.join(features)}"
    rows, cols = 1, len(features)

    fig = make_subplots(rows=rows, cols=cols, horizontal_spacing=0.1)

    axis_font = {"family": "Styrene A", "color": "#191919"}

    for i, feature in enumerate(features):
        fig.add_trace(
            go.Box(
                y=df[feature],
                marker={
                    "color": PRIMARY_COLORS[i % len(PRIMARY_COLORS)],
                    "line": {"color": "#000000", "width": 1},
                },
                boxmean="sd",
                showlegend=False,
            ),
            row=1,
            col=i + 1,
        )
        fig.update_yaxes(
            title_text="Value",
            row=1,
            col=i + 1,
            title_font={**axis_font, "size": 14},
            tickfont={**axis_font, "size": 12},
        )
        fig.update_xaxes(
            tickvals=[0],
            ticktext=[feature],
            row=1,
            col=i + 1,
            title_font={**axis_font, "size": 14},
            tickfont={**axis_font, "size": 12},
            showticklabels=True,
        )

    fig.update_layout(
        title_text=title,
        title_x=0.5,
        title_font={"family": "Styrene B", "size": 20, "color": "#191919"},
        showlegend=False,
        template="plotly_white",
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        height=500,
        width=400 * len(features),
        margin={"l": 50, "r": 50, "t": 80, "b": 150},
        font={**axis_font, "size": 12},
    )

    fig.show()

    if save_path:
        fig.write_image(save_path)


def plot_correlation_matrix(
    df: pd.DataFrame, numerical_features: List[str], save_path: str = None
) -> None:
    """Plots the correlation matrix of the specified numerical features in the DataFrame.

    Args:
        df (pd.DataFrame): DataFrame containing the data.
        numerical_features (List[str]): List of numerical features to include in the correlation matrix.
        save_path (str): Path to save the image file (optional).
    """
    numerical_df = df[numerical_features]
    correlation_matrix = numerical_df.corr()

    fig = px.imshow(
        correlation_matrix,
        text_auto=True,
        color_continuous_scale=PRIMARY_COLORS,
        title="Correlation Matrix",
    )

    fig.update_layout(
        title={
            "text": "Correlation Matrix",
            "y": 0.95,
            "x": 0.5,
            "xanchor": "center",
            "yanchor": "top",
        },
        title_font=dict(size=24),
        template="plotly_white",
        height=800,
        width=800,
        margin=dict(l=100, r=100, t=100, b=100),
        xaxis=dict(
            tickangle=-45, title_font=dict(size=18), tickfont=dict(size=14)
        ),
        yaxis=dict(title_font=dict(size=18), tickfont=dict(size=14)),
    )

    fig.show()

    if save_path:
        fig.write_image(save_path)


def plot_feature_importances(
    feature_importances: Dict[str, Dict[str, float]],
    save_path: Optional[str] = None,
) -> None:
    """Plots and optionally saves a bar chart of feature importances across different models.

    Args:
        feature_importances: A dictionary with model names as keys and dicts of feature importances as values.
        save_path: Optional path to save the plot image.

    Returns:
        None. Displays the plot and optionally saves it to a file.
    """
    fig = go.Figure()

    axis_font = {"family": "Styrene A", "color": "#191919"}

    for i, (name, importances) in enumerate(feature_importances.items()):
        fig.add_trace(
            go.Bar(
                x=list(importances.keys()),
                y=list(importances.values()),
                name=name,
                marker_color=PRIMARY_COLORS[i % len(PRIMARY_COLORS)],
                text=[f"{value:.3f}" for value in importances.values()],
                textposition="auto",
            )
        )

    fig.update_layout(
        title={
            "text": "Feature Importances Across Models",
            "y": 0.95,
            "x": 0.5,
            "xanchor": "center",
            "yanchor": "top",
            "font": {"family": "Styrene B", "size": 24, "color": "#191919"},
        },
        xaxis_title="Features",
        yaxis_title="Importance",
        barmode="group",
        template="plotly_white",
        legend_title="Models",
        font={**axis_font, "size": 14},
        height=600,
        width=1200,
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
    )

    fig.update_xaxes(tickangle=-45, tickfont={**axis_font, "size": 12})
    fig.update_yaxes(
        showgrid=True,
        gridwidth=1,
        gridcolor="LightGrey",
        tickfont={**axis_font, "size": 12},
    )

    fig.show()

    if save_path:
        fig.write_image(save_path)


def plot_distribution_comparison(
    df_before,
    df_after,
    features,
    title="Distribution Comparison",
    save_path=None,
):
    """
    Plots the distribution of specified features before and after handling missing values.

    Args:
        df_before (pd.DataFrame): DataFrame before handling missing values.
        df_after (pd.DataFrame): DataFrame after handling missing values.
        features (list): List of feature names to plot.
        title (str): The title of the plot.

    Returns:
        None. Displays the plot.
    """
    n_features = len(features)
    fig = make_subplots(
        rows=n_features,
        cols=2,
        subplot_titles=[f"{feature} - Before" for feature in features]
        + [f"{feature} - After" for feature in features],
    )

    for i, feature in enumerate(features):
        fig.add_trace(
            go.Histogram(
                x=df_before[feature],
                name="Before",
                marker_color=PRIMARY_COLORS[0],
            ),
            row=i + 1,
            col=1,
        )
        fig.add_trace(
            go.Histogram(
                x=df_after[feature],
                name="After",
                marker_color=PRIMARY_COLORS[1],
            ),
            row=i + 1,
            col=2,
        )

    fig.update_layout(
        title={
            "text": title,
            "y": 0.95,
            "x": 0.5,
            "xanchor": "center",
            "yanchor": "top",
            "font": {"size": 24, "color": "#191919", "family": "Styrene B"},
        },
        showlegend=False,
        template="plotly_white",
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        font={"family": "Styrene A", "size": 14, "color": "#191919"},
        height=400 * n_features,
        width=1200,
    )

    fig.show()

    if save_path:
        fig.write_image(save_path)


def plot_categorical_features_by_target(
    df: pd.DataFrame,
    features: List[str],
    target: str,
    save_path: Optional[str] = None,
) -> None:
    """
    Plot the distribution of specified categorical features grouped by a target variable.
    This function creates a grid of bar plots, where each plot represents the distribution
    of a categorical feature, grouped by the target variable. It shows percentages and
    uses a consistent y-axis scale across all subplots.

    Args:
        df (pd.DataFrame): The input DataFrame containing the data to be plotted.
        features (List[str]): A list of column names representing the categorical features to be plotted.
        target (str): The name of the target variable column used for grouping.
        save_path (Optional[str]): The file path to save the plot image. If None, the plot is not saved.

    Returns:
        None. The function displays the plot and optionally saves it to a file.
    """
    num_features = len(features)
    # Determine the number of rows and columns based on the number of features
    if num_features == 1:
        rows, cols = 1, 1
    elif num_features == 2:
        rows, cols = 1, 2
    elif num_features == 3:
        rows, cols = 2, 2
    else:
        rows, cols = (num_features + 1) // 2, 2

    fig = make_subplots(
        rows=rows,
        cols=cols,
        vertical_spacing=0.2,
        horizontal_spacing=0.1,
        # Removed subplot_titles to prevent feature names from appearing at the top
    )

    axis_font = {"family": "Styrene A", "color": "#191919"}
    colors = {0: PRIMARY_COLORS[0], 1: PRIMARY_COLORS[1]}

    for i, feature in enumerate(features):
        row, col = (i // cols) + 1, (i % cols) + 1
        data = (
            df.groupby([feature, target], observed=False)
            .size()
            .unstack(fill_value=0)
        )
        data_percentages = data.div(data.sum(axis=1), axis=0) * 100

        for category in data.columns:
            fig.add_trace(
                go.Bar(
                    x=data.index,
                    y=data_percentages[category],
                    name=f"{target} = {category}",
                    marker_color=colors[category],
                    text=[f"{v:.1f}%" for v in data_percentages[category]],
                    textposition="inside",
                    width=0.35,
                    showlegend=(i == 0),
                ),
                row=row,
                col=col,
            )

        fig.update_xaxes(
            title_text=feature,
            row=row,
            col=col,
            title_standoff=25,
            title_font={**axis_font, "size": 14},
            tickfont={**axis_font, "size": 12},
            showticklabels=True,
        )

        fig.update_yaxes(
            title_text="Percentage" if col == 1 else None,
            row=row,
            col=col,
            title_font={**axis_font, "size": 14},
            tickfont={**axis_font, "size": 12},
            range=[0, 100],
        )

    fig.update_layout(
        title_text=f"Distribution of {', '.join(features)} by {target}",
        title_x=0.5,
        title_font={"family": "Styrene B", "size": 20, "color": "#191919"},
        showlegend=True,
        template="plotly_white",
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        height=400 * rows,
        width=600 * cols,
        margin={"l": 50, "r": 150, "t": 100, "b": 50},
        font={**axis_font, "size": 12},
        legend=dict(
            orientation="v",
            yanchor="middle",
            y=0.5,
            xanchor="left",
            x=1.02,
            bgcolor="rgba(255,255,255,0.6)",
            bordercolor="Black",
            borderwidth=1,
            font={**axis_font, "size": 12},
        ),
    )

    fig.show()
    if save_path:
        fig.write_image(save_path)


def plot_numeric_distributions(
    df: pd.DataFrame,
    features: List[str],
    target: str,
    nbins: int = 40,
    save_path: Optional[str] = None,
) -> None:
    """Plots numerical distribution for specified features in the DataFrame.

    Shows distributions for overall (sum of target 0 and 1), target = 0, and target = 1.
    Bars are plotted with overlap, using edge highlighting for distinction.
    Legend is placed on the right side with colors unaffected by opacity.

    Args:
        df: DataFrame containing the features and target variable.
        features: List of feature names to plot distributions for.
        target: Name of the binary target variable column.
        nbins: Number of bins for each histogram. Defaults to 40.
        save_path: Optional path to save the plot image.

    Returns:
        None. Displays the plot and optionally saves it to a file.
    """
    rows, cols = 1, len(features)
    fig = make_subplots(rows=rows, cols=cols, horizontal_spacing=0.05)
    axis_font = {"family": "Styrene A", "color": "#191919"}
    plot_colors = PLOT_COLORS
    categories = ["Overall", "Target = 1", "Target = 0"]

    for i, feature in enumerate(features):
        hist_data = []
        bin_edges = None
        for category in [0, 1]:
            x = df[df[target] == category][feature]
            hist, edges = np.histogram(
                x, bins=nbins, range=(df[feature].min(), df[feature].max())
            )
            hist_data.append(hist)
            if bin_edges is None:
                bin_edges = edges

        overall_hist = hist_data[0] + hist_data[1]
        hist_data = [overall_hist] + hist_data[::-1]

        bin_width = (bin_edges[-1] - bin_edges[0]) / nbins

        for data, color, name in zip(hist_data, plot_colors, categories):
            # Main bar with reduced opacity
            fig.add_trace(
                go.Bar(
                    x=bin_edges[:-1],
                    y=data,
                    name=name,
                    marker_color=color,
                    opacity=0.1,
                    showlegend=False,
                    width=bin_width,
                ),
                row=1,
                col=i + 1,
            )

            # Edge highlight
            fig.add_trace(
                go.Scatter(
                    x=bin_edges[:-1],
                    y=data,
                    mode="lines",
                    line=dict(color=color, width=2),
                    showlegend=(i == 0),
                    name=name,
                    hoverinfo="skip",
                ),
                row=1,
                col=i + 1,
            )

        fig.update_xaxes(
            title_text=feature,
            row=1,
            col=i + 1,
            title_font={**axis_font, "size": 14},
            tickfont={**axis_font, "size": 12},
        )

        if i == 0:
            fig.update_yaxes(
                title_text="Count",
                row=1,
                col=1,
                title_font={**axis_font, "size": 14},
                tickfont={**axis_font, "size": 12},
            )

    fig.update_layout(
        title_text=f"Distribution of {', '.join(features)} by {target}",
        title_x=0.5,
        title_font={"family": "Styrene B", "size": 20, "color": "#191919"},
        showlegend=True,
        legend=dict(
            orientation="v",
            yanchor="middle",  # Changed from "top" to "middle"
            y=0.5,  # Changed from 1 to 0.5 for vertical center
            xanchor="left",
            x=1.02,
            bgcolor="rgba(255,255,255,0.8)",
        ),
        template="plotly_white",
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        height=600,
        width=400 * len(features) + 150,
        margin={
            "l": 50,
            "r": 150,
            "t": 80,
            "b": 80,
        },
        font={**axis_font, "size": 12},
        barmode="overlay",
        bargap=0,
        bargroupgap=0,
    )

    fig.show()
    if save_path:
        fig.write_image(save_path)


def plot_single_bar_chart(
    df: pd.DataFrame,
    feature: str,
    save_path: Optional[str] = None,
) -> None:
    """Plots a percentage bar chart for a specified categorical feature in the DataFrame.

    Args:
        df: DataFrame containing the feature to plot.
        feature: Name of the categorical feature to plot.
        save_path: Optional path to save the plot image.

    Returns:
        None. Displays the plot and optionally saves it to a file.
    """
    title = f"Distribution of target variable {feature}"

    value_counts = df[feature].value_counts(normalize=True).reset_index()
    value_counts.columns = [feature, "percentage"]
    value_counts["percentage"] *= 100  # Convert to percentage

    fig = go.Figure()

    for i, (value, percentage) in enumerate(
        zip(value_counts[feature], value_counts["percentage"])
    ):
        fig.add_trace(
            go.Bar(
                x=[value],
                y=[percentage],
                name=f"{feature} = {value}",  # Updated legend label
                marker_color=PRIMARY_COLORS[
                    i % 2
                ],  # Alternate between first two colors
                text=[f"{percentage:.1f}%"],
                textposition="auto",
            )
        )

    axis_font = {"family": "Styrene A", "color": "#191919"}

    fig.update_xaxes(
        title_text=feature,
        title_standoff=25,
        title_font={**axis_font, "size": 14},
        tickfont={**axis_font, "size": 12},
        showticklabels=True,
    )

    fig.update_yaxes(
        title_text="Percentage",
        title_font={**axis_font, "size": 14},
        tickfont={**axis_font, "size": 12},
        range=[0, 100],  # Set y-axis range from 0 to 100
    )

    fig.update_layout(
        title_text=title,
        title_x=0.5,
        title_font={"family": "Styrene B", "size": 20, "color": "#191919"},
        showlegend=True,
        template="plotly_white",
        plot_bgcolor=BACKGROUND_COLOR,
        paper_bgcolor=BACKGROUND_COLOR,
        height=500,
        width=1200,
        margin={"l": 50, "r": 150, "t": 80, "b": 50},
        font={**axis_font, "size": 12},
        barmode="group",
        legend=dict(
            orientation="v",
            yanchor="middle",
            y=0.5,
            xanchor="left",
            x=1.02,
            bgcolor="rgba(255,255,255,0.6)",
            bordercolor="Black",
            borderwidth=1,
            font={**axis_font, "size": 12},
        ),
    )

    fig.show()

    if save_path:
        fig.write_image(save_path)


=== src/retail_bank_risk/data_preprocessing_utils.py ===
# cspell:disable
# pylint:disable=line-too-long
"""
This module provides various functions for data preprocessing, anomaly detection, feature engineering,
and statistical analysis, specifically tailored for machine learning workflows.

Functions included:
- `detect_anomalies_iqr`: Detects anomalies in multiple features using the Interquartile Range (IQR) method.
- `flag_anomalies`: Flags anomalies in specified features using the IQR method.
- `calculate_cramers_v`: Computes Cramer's V statistic for categorical-categorical association.
- `handle_missing_values`: Handles missing data by dropping columns or rows based on a threshold.
- `simple_imputation`: Performs simple imputation on missing values in training and testing datasets.
- `engineer_spaceship_features`: Performs feature engineering, particularly for a spaceship passenger dataset.
- `confidence_interval`: Calculates the confidence interval for a given dataset.
- `create_pipeline`: Creates a scikit-learn pipeline for preprocessing and modeling.

This module is intended for use in data preprocessing and feature engineering, with functions designed to handle common
tasks in machine learning pipelines, such as anomaly detection, missing data handling, and feature engineering.
"""


from collections import Counter
from typing import List, Tuple

import numpy as np
import pandas as pd
import polars as pl
from scipy import stats
from sklearn.pipeline import Pipeline
from sklearn.impute import KNNImputer


def reduce_memory_usage_pl(
    df: pl.DataFrame, verbose: bool = True
) -> pl.DataFrame:
    """Reduces memory usage of a Polars DataFrame by optimizing data types.

    This function attempts to downcast numeric columns to the smallest possible
    data type that can represent the data without loss of information. It also
    converts string columns to categorical type.

    Args:
        df: A Polars DataFrame to optimize.
        verbose: If True, print memory usage before and after optimization.

    Returns:
        A Polars DataFrame with optimized memory usage.

    References:
        Adapted from:
        https://www.kaggle.com/code/demche/polars-memory-usage-optimization
        Original pandas version:
        https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65
    """
    if verbose:
        print(f"Size before memory reduction: {df.estimated_size('mb'):.2f} MB")
        print(f"Initial data types: {Counter(df.dtypes)}")

    numeric_int_types = [pl.Int8, pl.Int16, pl.Int32, pl.Int64]
    numeric_float_types = [pl.Float32, pl.Float64]

    for col in df.columns:
        col_type = df[col].dtype

        if col_type in numeric_int_types:
            c_min = df[col].min() * 10  # Prevent possible integer overflow
            c_max = df[col].max() * 10

            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                new_type = pl.Int8
            elif (
                c_min > np.iinfo(np.int16).min
                and c_max < np.iinfo(np.int16).max
            ):
                new_type = pl.Int16
            elif (
                c_min > np.iinfo(np.int32).min
                and c_max < np.iinfo(np.int32).max
            ):
                new_type = pl.Int32
            else:
                new_type = pl.Int64

            df = df.with_columns(df[col].cast(new_type))

        elif col_type in numeric_float_types:
            c_min, c_max = df[col].min(), df[col].max()
            if (
                c_min > np.finfo(np.float32).min
                and c_max < np.finfo(np.float32).max
            ):
                df = df.with_columns(df[col].cast(pl.Float32))

        elif col_type == pl.String:
            df = df.with_columns(df[col].cast(pl.Categorical))

    if verbose:
        print(f"Size after memory reduction: {df.estimated_size('mb'):.2f} MB")
        print(f"Final data types: {Counter(df.dtypes)}")

    return df


def initial_feature_reduction(
    train_df: pl.DataFrame,
    test_df: pl.DataFrame,
    target_col: str,
    missing_threshold: float = 0.5,
    variance_threshold: float = 0.01,
    correlation_threshold: float = 0.05,
    essential_features: List[str] = None,
) -> Tuple[pl.DataFrame, pl.DataFrame]:
    """Reduces features based on missing values, variance, and correlation.

    This function performs feature reduction on the input DataFrames based on
    missing values, variance, and correlation with the target variable, while
    ensuring that essential features are always retained.

    Args:
        train_df: Training DataFrame.
        test_df: Testing DataFrame.
        target_col: The name of the target variable column.
        missing_threshold: Max allowable missing value ratio. Defaults to 0.5.
        variance_threshold: Min variance required. Defaults to 0.01.
        correlation_threshold: Min absolute correlation with target.
            Defaults to 0.05.
        essential_features: List of features to always keep, regardless of
            reduction criteria. Defaults to None.

    Returns:
        A tuple containing the reduced train and test DataFrames.

    Raises:
        ValueError: If the target column is not in the training DataFrame.
    """
    if target_col not in train_df.columns:
        raise ValueError(
            f"Target column '{target_col}' not found in training data."
        )

    essential_features = essential_features or []

    combined_df = pl.concat([train_df.drop(target_col), test_df])
    total_rows = len(combined_df)

    cols_to_keep_missing = _filter_by_missing_values(
        combined_df.drop(essential_features), total_rows, missing_threshold
    )
    filtered_df = combined_df.select(cols_to_keep_missing + essential_features)

    numeric_cols = _get_numeric_columns(filtered_df)
    numeric_cols = [
        col for col in numeric_cols if col not in essential_features
    ]

    cols_to_keep_variance = _filter_by_variance(
        filtered_df, numeric_cols, variance_threshold
    )

    cols_to_keep_correlation = _filter_by_correlation(
        train_df, numeric_cols, target_col, correlation_threshold
    )

    final_cols = list(
        set(cols_to_keep_missing)
        & set(cols_to_keep_variance)
        & set(cols_to_keep_correlation)
    )
    final_cols.extend(
        [col for col in cols_to_keep_missing if col not in numeric_cols]
    )
    final_cols.extend(essential_features)

    return (
        train_df.select([target_col] + final_cols),
        test_df.select(final_cols),
    )


def impute_numerical_features(train_df, test_df, target_col):
    """Imputes missing values in numerical features using KNNImputer.

    Args:
        train_df: Polars DataFrame for the training set.
        test_df: Polars DataFrame for the test set.
        target_col: Name of the target column in the training set.

    Returns:
        Tuple of imputed training and test DataFrames.
    """
    if not train_df.shape[0] or not test_df.shape[0]:
        print(
            "Warning: One of the DataFrames is empty. Skipping numerical imputation."
        )
        return train_df, test_df

    numerical_features = [
        col
        for col, dtype in zip(train_df.columns, train_df.dtypes)
        if dtype
        in (pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64)
        and col != target_col
    ]

    if not numerical_features:
        print(
            "Warning: No numerical features found. Skipping numerical imputation."
        )
        return train_df, test_df

    imputer = KNNImputer(n_neighbors=5)

    train_target = train_df[target_col]
    train_features = train_df.drop(target_col)

    train_numerical_pd = train_features[numerical_features].to_pandas()
    test_numerical_pd = test_df[numerical_features].to_pandas()

    train_numerical_imputed = imputer.fit_transform(train_numerical_pd)
    test_numerical_imputed = imputer.transform(test_numerical_pd)

    train_numerical_imputed_df = pl.DataFrame(
        train_numerical_imputed, schema=numerical_features
    )
    test_numerical_imputed_df = pl.DataFrame(
        test_numerical_imputed, schema=numerical_features
    )

    train_df = (
        train_features.drop(numerical_features)
        .hstack(train_numerical_imputed_df)
        .with_columns(train_target)
    )

    test_df = test_df.drop(numerical_features).hstack(test_numerical_imputed_df)

    return train_df, test_df


def impute_categorical_features(train_df, test_df, target_col):
    """Imputes missing values in categorical features using the mode.

    Args:
        train_df: Polars DataFrame for the training set.
        test_df: Polars DataFrame for the test set.
        target_col: Name of the target column in the training set.

    Returns:
        Tuple of imputed training and test DataFrames.
    """
    if not train_df.shape[0] or not test_df.shape[0]:
        print(
            "Warning: One of the DataFrames is empty. Skipping categorical imputation."
        )
        return train_df, test_df

    categorical_features = [
        col
        for col, dtype in zip(train_df.columns, train_df.dtypes)
        if dtype == pl.Categorical and col != target_col
    ]

    if not categorical_features:
        print("Warning: No categorical features found. Skipping imputation.")
        return train_df, test_df

    for col in categorical_features:
        train_df = train_df.with_columns(pl.col(col).fill_null("mode"))
        test_df = test_df.with_columns(pl.col(col).fill_null("mode"))

    return train_df, test_df


def count_duplicated_rows(dataframe: pl.DataFrame) -> None:
    """
    Count and print the number of duplicated rows in a Polars DataFrame
    (based on all columns).
    """
    num_duplicated_rows = dataframe.is_duplicated().sum()
    print(f"The DataFrame contains {num_duplicated_rows} duplicated rows.")


def _filter_by_missing_values(
    df: pl.DataFrame, total_rows: int, threshold: float
) -> List[str]:
    """Filters columns based on missing value ratio.

    Args:
        df: DataFrame to filter.
        total_rows: Total number of rows in the DataFrame.
        threshold: Maximum allowed ratio of missing values.

    Returns:
        List of column names that pass the missing value filter.
    """
    missing_ratios = df.null_count() / total_rows
    return [
        col
        for col, ratio in zip(df.columns, missing_ratios.to_numpy()[0])
        if ratio <= threshold
    ]


def _get_numeric_columns(df: pl.DataFrame) -> List[str]:
    """Returns a list of numeric column names.

    Args:
        df: DataFrame to analyze.

    Returns:
        List of column names with numeric data types.
    """
    numeric_types = (
        pl.Float32,
        pl.Float64,
        pl.Int8,
        pl.Int16,
        pl.Int32,
        pl.Int64,
    )
    return [col for col in df.columns if df[col].dtype in numeric_types]


def _filter_by_variance(
    df: pl.DataFrame, columns: List[str], threshold: float
) -> List[str]:
    """Filters columns based on variance.

    Args:
        df: DataFrame containing the columns to filter.
        columns: List of column names to consider.
        threshold: Minimum required variance.

    Returns:
        List of column names that pass the variance filter.
    """
    variances = df.select(columns).var().to_dict(as_series=False)
    return [col for col, var in variances.items() if var[0] > threshold]


def _filter_by_correlation(
    df: pl.DataFrame, columns: List[str], target_col: str, threshold: float
) -> List[str]:
    """Filters columns based on correlation with the target variable.

    Args:
        df: DataFrame containing the columns and target variable.
        columns: List of column names to consider.
        target_col: Name of the target column.
        threshold: Minimum required absolute correlation.

    Returns:
        List of column names that pass the correlation filter.
    """

    def correlation_with_target(col: str) -> float:
        """Calculates absolute Pearson correlation with the target."""
        x = df[col].to_numpy()
        y = df[target_col].to_numpy()
        mask = ~np.isnan(x) & ~np.isnan(y)
        return np.abs(np.corrcoef(x[mask], y[mask])[0, 1])

    correlations = {col: correlation_with_target(col) for col in columns}
    return ["amt_income_total"] + [
        col
        for col, corr in correlations.items()
        if corr > threshold and col != "amt_income_total"
    ]


def detect_anomalies_iqr(df: pd.DataFrame, features: List[str]) -> pd.DataFrame:
    """Detects anomalies in multiple features using the IQR method.

    Args:
        df (pd.DataFrame): DataFrame containing the data.
        features (List[str]): List of features to detect anomalies in.

    Returns:
        pd.DataFrame: DataFrame containing the anomalies for each feature.
    """
    anomalies_list = []

    for feature in features:
        if feature not in df.columns:
            print(f"Feature '{feature}' not found in DataFrame.")
            continue

        if not np.issubdtype(df[feature].dtype, np.number):
            print(f"Feature '{feature}' is not numerical and will be skipped.")
            continue

        q1 = df[feature].quantile(0.25)
        q3 = df[feature].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        feature_anomalies = df[
            (df[feature] < lower_bound) | (df[feature] > upper_bound)
        ]
        if not feature_anomalies.empty:
            print(f"Anomalies detected in feature '{feature}':")
            print(feature_anomalies)
        else:
            print(f"No anomalies detected in feature '{feature}'.")
        anomalies_list.append(feature_anomalies)

    if anomalies_list:
        anomalies = (
            pd.concat(anomalies_list).drop_duplicates().reset_index(drop=True)
        )
        anomalies = anomalies[features]
    else:
        anomalies = pd.DataFrame(columns=features)

    return anomalies


def flag_anomalies(df: pd.DataFrame, features: List[str]) -> pd.Series:
    """
    Identify and flag anomalies in a DataFrame based on the Interquartile Range (IQR) method for specified features.

    Args:
        df (pd.DataFrame): The input DataFrame containing the data.
        features (List[str]): A list of column names in the DataFrame to check for anomalies.

    Returns:
        pd.Series: A Series of boolean values where True indicates an anomaly in any of the specified features.
    """
    anomaly_flags = pd.Series(False, index=df.index)

    for feature in features:
        first_quartile = df[feature].quantile(0.25)
        third_quartile = df[feature].quantile(0.75)
        interquartile_range = third_quartile - first_quartile
        lower_bound = first_quartile - 1.5 * interquartile_range
        upper_bound = third_quartile + 1.5 * interquartile_range

        feature_anomalies = (df[feature] < lower_bound) | (
            df[feature] > upper_bound
        )
        anomaly_flags |= feature_anomalies

    return anomaly_flags


def calculate_cramers_v(x, y):
    """
    Calculates Cramer's V statistic for categorical-categorical association.

    Args:
        x: pandas Series
        y: pandas Series

    Returns:
        float: Cramer's V
    """
    confusion_matrix = pd.crosstab(x, y)
    chi2 = stats.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum().sum()
    min_dim = min(confusion_matrix.shape) - 1
    return np.sqrt(chi2 / (n * min_dim))


def get_top_missing_value_percentages(
    df: pl.DataFrame, top_n: int = 5
) -> pl.DataFrame:
    """Calculates the percentage of missing values for each column in a Polars DataFrame
    and returns the top N columns with the highest percentage of missing values.

    Args:
        df: The Polars DataFrame to analyze for missing values.
        top_n: The number of top columns to return (default is 5).

    Returns:
        pl.DataFrame: A Polars DataFrame with columns for column names and missing
        value percentages.
    """
    total_rows = df.height
    missing_counts = df.null_count().row(0)
    missing_percentages = [
        {"column": col, "missing_percentage": count / total_rows * 100}
        for col, count in zip(df.columns, missing_counts)
        if count > 0
    ]

    if missing_percentages:
        missing_df = pl.DataFrame(missing_percentages)
        return (
            missing_df.sort("missing_percentage", descending=True)
            .with_columns(pl.col("missing_percentage").round(2))
            .head(top_n)
        )
    else:
        return pl.DataFrame()


def analyze_missing_values(
    train_df: pl.DataFrame, test_df: pl.DataFrame, top_n: int = 5
) -> None:
    """Analyzes and prints the top N columns with the highest percentage of missing values
    for both train and test DataFrames.

    Args:
        train_df: The training Polars DataFrame.
        test_df: The testing Polars DataFrame.
        top_n: The number of top columns to display (default is 5).
    """
    train_missing = get_top_missing_value_percentages(train_df, top_n)
    test_missing = get_top_missing_value_percentages(test_df, top_n)

    if train_missing.is_empty():
        print("No missing values found in the reduced train set.")
    else:
        print(f"Top {top_n} columns with missing values in reduced train set:")
        print(train_missing)

    if test_missing.is_empty():
        print("No missing values found in the reduced test set.")
    else:
        print(f"\nTop {top_n} columns with missing values in reduced test set:")
        print(test_missing)


def confidence_interval(
    data: List[float], confidence: float = 0.95
) -> Tuple[float, float, float]:
    """Calculates the confidence interval for a given dataset.

    Args:
        data (List[float]): A list of numerical data points.
        confidence (float): The confidence level for the interval. Defaults to 0.95.

    Returns:
        Tuple[float, float, float]: A tuple containing the mean, lower bound, and upper bound of the confidence interval.
    """
    array_data = np.array(data, dtype=float)
    sample_size = len(array_data)
    mean_value = np.mean(array_data)
    standard_error = stats.sem(array_data)
    margin_of_error = standard_error * stats.t.ppf(
        (1 + confidence) / 2.0, sample_size - 1
    )
    return (
        mean_value,
        mean_value - margin_of_error,
        mean_value + margin_of_error,
    )


def create_pipeline(preprocessor: Pipeline, model: Pipeline) -> Pipeline:
    """
    Create a machine learning pipeline with a preprocessor and a classifier.

    Parameters:
    preprocessor (sklearn.base.TransformerMixin): The preprocessing component of the pipeline.
    model (sklearn.base.BaseEstimator): The classifier component of the pipeline.

    Returns:
    sklearn.pipeline.Pipeline: A scikit-learn Pipeline object that sequentially applies the preprocessor and the classifier.
    """
    return Pipeline([("preprocessor", preprocessor), ("classifier", model)])


=== src/retail_bank_risk/model_utils.py ===
# cspell:disable
# pylint:disable=line-too-long

"""
This module provides functions for evaluating machine learning models and extracting feature importances.

Functions included:
- `evaluate_model`: Evaluates the performance of a trained model using various metrics, including optional threshold adjustment.
- `extract_feature_importances`: Extracts feature importances using permutation importance for models that do not provide them directly.

Key Features:
- Evaluation metrics include ROC AUC, PR AUC, F1 Score, Precision, Recall, and Balanced Accuracy.
- The `evaluate_model` function allows for adjusting the decision threshold based on a target recall.
- The `extract_feature_importances` function uses permutation importance for models that lack direct feature importance attributes.

This module is intended for use in machine learning workflows to assess model performance and interpret feature importance.
"""

from typing import Dict, Union

import numpy as np
import pandas as pd
from sklearn.inspection import permutation_importance
from sklearn.metrics import (
    average_precision_score,
    balanced_accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_recall_curve,
    precision_score,
    recall_score,
    roc_auc_score,
)


def evaluate_model(
    model,
    features: np.ndarray,
    true_labels: np.ndarray,
    dataset_name: str = None,
    threshold: float = None,
    target_recall: float = None,
) -> Dict[str, Union[float, np.ndarray]]:
    """
    Evaluate a model's performance with optional threshold adjustment.

    Args:
        model: The trained model to evaluate.
        features (np.ndarray): Feature data.
        true_labels (np.ndarray): True labels.
        dataset_name (str, optional): Name of the dataset for display purposes.
        threshold (float, optional): Custom threshold for classification.
        target_recall (float, optional): Target recall for threshold adjustment.

    Returns:
        Dict[str, Union[float, np.ndarray]]: Dictionary containing various performance metrics.
    """
    y_pred_proba = model.predict_proba(features)[:, 1]

    if target_recall is not None:
        _, recalls, thresholds = precision_recall_curve(
            true_labels, y_pred_proba
        )
        idx = np.argmin(np.abs(recalls - target_recall))
        threshold = thresholds[idx]
        print(f"Adjusted threshold: {threshold:.4f}")

    # Use the threshold if provided or adjusted, otherwise use default 0.5
    if threshold is not None:
        y_pred = (y_pred_proba >= threshold).astype(int)
    else:
        y_pred = model.predict(features)

    if dataset_name:
        print(f"\nResults on {dataset_name} set:")

    print(
        classification_report(true_labels, y_pred, zero_division=1)
    )  # Modified line
    print("Confusion Matrix:")
    print(confusion_matrix(true_labels, y_pred))
    print(f"ROC AUC: {roc_auc_score(true_labels, y_pred_proba):.4f}")
    print(f"PR AUC: {average_precision_score(true_labels, y_pred_proba):.4f}")
    print(f"F1 Score: {f1_score(true_labels, y_pred, zero_division=1):.4f}")
    print(
        f"Precision: {precision_score(true_labels, y_pred, zero_division=1):.4f}"
    )
    print(f"Recall: {recall_score(true_labels, y_pred):.4f}")
    print(
        f"Balanced Accuracy: {balanced_accuracy_score(true_labels, y_pred):.4f}"
    )

    return {
        "roc_auc": roc_auc_score(true_labels, y_pred_proba),
        "pr_auc": average_precision_score(true_labels, y_pred_proba),
        "f1": f1_score(true_labels, y_pred, zero_division=1),
        "precision": precision_score(true_labels, y_pred, zero_division=1),
        "recall": recall_score(true_labels, y_pred),
        "balanced_accuracy": balanced_accuracy_score(true_labels, y_pred),
        "threshold": threshold if threshold is not None else 0.5,
        "y_pred": y_pred,
        "y_pred_proba": y_pred_proba,
    }


def extract_feature_importances(
    model, feature_data: pd.DataFrame, target_data: Union[pd.Series, np.ndarray]
) -> np.ndarray:
    """
    Extract feature importances using permutation importance for models that do not directly provide them.

    Args:
        model: Trained model.
        feature_data (pd.DataFrame): Feature data.
        target_data (Union[pd.Series, np.ndarray]): Target data.

    Returns:
        np.ndarray: Array of feature importances.
    """
    if hasattr(model, "feature_importances_"):
        return model.feature_importances_
    else:
        # Calculate permutation importance
        permutation_import = permutation_importance(
            model, feature_data, target_data, n_repeats=30, random_state=42
        )
        return permutation_import.importances_mean


